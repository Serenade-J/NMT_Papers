# NMT_Papers

## Recommended Papers for NMT (Neural Machine Translation)

Some good(maybe) papers about NMT.

### Model:

1. **Sequence to Sequence Learning with Neural Networks.**
*Ilya Sutskever, Oriol Vinyals, Quoc V. Le.* NIPS 2014. [paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

1. **Neural Machine Translation by Jointly Learning to Align and Translate.**
*Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.* ICLR 2015. [paper](https://arxiv.org/pdf/1409.0473.pdf)

1. **Effective Approaches to Attention-based Neural Machine Translation.**
*Minh-Thang Luong, Hieu Pham, Christopher D. Manning.* EMNLP 2015. [paper](http://aclweb.org/anthology/D15-1166)

1. **Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.**
*Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean.* 2016.  [paper](https://arxiv.org/pdf/1609.08144.pdf)

1. **Convolutional Sequence to Sequence Learning.**
*Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin.*  2017.  [paper](https://arxiv.org/pdf/1705.03122.pdf)

1. **Attention Is All You Need.**
*Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.* NIPS 2017.  [paper](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)

### Analysis:

1. **Massive Exploration of Neural Machine Translation Architectures.**
*Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le.* ACL 2017.  [paper](https://www.aclweb.org/anthology/D17-1151)

1. **How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures.**
*Tobias Domhan.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1167)

1. **Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures.**
*Gongbo Tang, Mathias Müller, Annette Rios, Rico Sennrich.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1458)

1. **Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter.**
*Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1396)

### Context Model:

### Multi Task NMT:

### Low Scource:

### Multilingual NMT

### unsupervised NMT:


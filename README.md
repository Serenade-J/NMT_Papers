# NMT_Papers (Update Continuously)

## Recommended Papers for NMT (Neural Machine Translation)

Some papers about NMT.

I have read all the papers. Hope you can fine something you need(like)!

### Must Reads:

* **Sequence to Sequence Learning with Neural Networks.**
*Ilya Sutskever, Oriol Vinyals, Quoc V. Le.* NIPS 2014. [paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

* **Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.**
*Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio.* EMNLP 2014. [paper](https://www.aclweb.org/anthology/D14-1179)

* **Neural Machine Translation by Jointly Learning to Align and Translate.**
*Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.* ICLR 2015. [paper](https://arxiv.org/pdf/1409.0473.pdf)

* **Effective Approaches to Attention-based Neural Machine Translation.**
*Minh-Thang Luong, Hieu Pham, Christopher D. Manning.* EMNLP 2015. [paper](http://aclweb.org/anthology/D15-1166)

* **Convolutional Sequence to Sequence Learning.**
*Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin.*  2017.  [paper](https://arxiv.org/pdf/1705.03122.pdf)

* **Attention Is All You Need.**
*Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, Macduff Hughes.* ACL 2018.  [paper](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)

* **BLEU: a Method for Automatic Evaluation of Machine Translation.**
*Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu.* ACL 2002.  [paper](https://www.aclweb.org/anthology/P02-1040.pdf)

* **Neural Machine Translation of Rare Words with Subword Units.**
*Rico Sennrich and Barry Haddow and Alexandra Birch.* ACL 2016.  [paper](http://www.aclweb.org/anthology/P16-1162)



### Analysis:

* **Does String-Based Neural MT Learn Source Syntax?.**
*Xing Shi, Inkit Padhi, and Kevin Knight.* EMNLP 2016.  [paper](http://aclweb.org/anthology/D16-1159)

* **What do Neural Machine Translation Models Learn about Morphology?**
*Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass.* ACL 2017.  [paper](https://arxiv.org/pdf/1704.0347*pdf)

* **Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks.**
*Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, James Glass.* ICJNLP 2017.  [paper](http://aclweb.org/anthology/I17-1001)

* **Massive Exploration of Neural Machine Translation Architectures.**
*Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le.* ACL 2017.  [paper](https://www.aclweb.org/anthology/D17-1151)

* **How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures.**
*Tobias Domhan.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1167)

* **Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures.**
*Gongbo Tang, Mathias Müller, Annette Rios, Rico Sennrich.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1458)

* **Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter.**
*Lijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1396)

* **The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation.**
*Arianna Bisazza, Clara Tump.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1313)

* **An Analysis of Encoder Representations in Transformer-Based Machine Translation.**
*Alessandro Raganato and Jorg Tiedemann.* EMNLP Worshopp BlackboxNLP 2018.  [paper](http://aclweb.org/anthology/W18-5431)

* **When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?**
*Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Janani Padmanabhan, Graham Neubig.* NAACL 2018.  [paper](http://www.aclweb.org/anthology/N18-2084)

* **On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference**
*Adam Poliak, Yonatan Belinkov, James Glass, Benjamin Van Durme.* NAACL 2018.  [paper](http://www.aclweb.org/anthology/N18-2082)

* **A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation.**
*Surafel M. Lakew, Mauro Cettolo, Marcello Federico.* COLING 2018.  [paper](http://aclweb.org/anthology/C18-1054)

* **On the Impact of Various Types of Noise on Neural Machine Translation.**
*Huda Khayrallah, Philipp Koehn.* WNMT 2018.  [paper](http://www.aclweb.org/anthology/W18-2709)

* **Context and Copying in Neural Machine Translation.**
*Rebecca Knowles, Philipp Koehn.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1339)

### Attention :

#### RNN seq2seq:

* **Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.**
*Junyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, Qi Su.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1331)

* **Surprisingly Easy Hard-Attention for Sequence to Sequence Learning.**
*Shiv Shankar, Siddhant Garg, Sunita Sarawagi.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1065)

* **Sparse and Constrained Attention for Neural Machine Translation.**
*Chaitanya Malaviya, Pedro Ferreira, André F. T. Martins.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-2059)

* **Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings.**
*Shaohui Kuang, Junhui Li, António Branco, Weihua Luo, Deyi Xiong.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1164)

#### Transformer:

* **Accelerating Neural Transformer via an Average Attention Network.**
*Biao Zhang, Deyi Xiong, Jinsong Su.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1166)

* **Modeling Localness for Self-Attention Networks.**
*Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, Tong Zhang.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1475)

* **On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.**
*Tamer Alkhouli, Gabriel Bretschner, Hermann Ney.* WMT 2018.  [paper](http://aclweb.org/anthology/W18-6318)

* **Multi-Head Attention with Disagreement Regularization.**
*Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, Tong Zhang.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1317)

### Context and Document-level NMT:

* **Neural Machine Translation with Extended Context.**
*Jorg Tiedemann and Yves Scherrer.* DiscoMT 2017.  [paper](http://www.aclweb.org/anthology/W17-4811)

* **Context Gates for Neural Machine Translation.**
*Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu, Hang Li.* TACL 2017.  [paper](http://www.aclweb.org/anthology/Q17-1007)

* **Does Neural Machine Translation Benefit from Larger Context?.**
*Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho.* EMNLP 2017.  [paper](https://arxiv.org/pdf/1704.05135.pdf)

* **Exploiting Cross-Sentence Context for Neural Machine Translation.**
*Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu.* EMNLP 2017.  [paper](http://aclweb.org/anthology/D17-1301)

* **Evaluating Discourse Phenomena in Neural Machine Translation.**
*Rachel Bawden, Rico Sennrich, Alexandra Birch, Barry Haddow.* NAACL 2018.  [paper](http://aclweb.org/anthology/N18-1118)

* **Context-Aware Neural Machine Translation Learns Anaphora Resolution.** *Elena Voita, Pavel Serdyukov, Rico Sennrich, Ivan Titov.* ACL 2018.  [paper](http://www.aclweb.org/anthology/P18-1117)

* **Exploiting Cross-Sentence Context for Neural Machine Translation.**
*Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu.* EMNLP 2017.  [paper](http://aclweb.org/anthology/D17-1301)

* **Document Context Neural Machine Translation with Memory Networks.**
*Sameen Maruf, Gholamreza Haffari.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1118)

* **Improving the Transformer Translation Model with Document-Level Context.**
*Jiacheng Zhang, Huanbo Luan, Maosong Sun, FeiFei Zhai, Jingfang Xu, Min Zhang, Yang Liu.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1049)

* **Document-Level Neural Machine Translation with Hierarchical Attention Networks.**
*Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1325)

* **Context-Dependent Word Representation for Neural Machine Translation.**
*Heeyoul Choi, Kyunghyun Cho, Yoshua Bengio.*   [paper](https://arxiv.org/pdf/1607.00578.pdf)

* **Handling Homographs in Neural Machine Translation.**
*Frederick Liu, Han Lu, Graham Neubig.* NAACL 2018.  [paper](http://aclweb.org/anthology/N18-1121)

### Memory Augmented NMT:

* **Memory-enhanced Decoder for Neural Machine Translation.**
*Mingxuan Wang, Zhengdong Lu, Hang Li, Qun Liu.* EMNLP 2016.  [paper](https://aclweb.org/anthology/D16-1027)

* **Memory-augmented Neural Machine Translation.**
*Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang, Andrew Abel.* EMNLP 2017.  [paper](https://www.aclweb.org/anthology/D17-1146)

* **Neural Machine Translation with Recurrent Attention Modeling.**
*Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, Alex Smola.* EACL 2017.  [paper](http://www.aclweb.org/anthology/E17-2061)

* **Learning to Remember Translation History with a Continuous Cache.**
*haopeng Tu, Yang Liu, Shuming Shi, Tong Zhang.* TACL 2018.  [paper](http://aclweb.org/anthology/Q18-1029)

* **Neural Machine Translation with Key-Value Memory-Augmented Attention.**
*Fandong Meng, Zhaopeng Tu, Yong Cheng, Haiyang Wu, Junjie Zhai, Yuekui Yang, Di Wang.* IJCAI 2018.  [paper](https://www.ijcai.org/proceedings/2018/0357.pdf)

### Domain adaptation:

* **Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination.**
*Jiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun Xie, Yongjing Yin, Jianqiang Zhao.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1041)

* **Extreme Adaptation for Personalized Neural Machine Translation.**
*Paul Michel, Graham Neubig.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-2050)

* **Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.**
*David Vilar.* NAACL 2018.  [paper](http://www.aclweb.org/anthology/N18-2080)


### Multi Task/Source NMT:

* **Multi-Source Neural Translation.**
*Barret Zoph and Kevin Knight.* NAACL 2016.  [paper](http://www.aclweb.org/anthology/N16-1004)

* **Multi-task Sequence to Sequence Learning.**
*Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser.* ICLR 2016.  [paper](https://arxiv.org/pdf/151*06114.pdf)

* **Attention Strategies for Multi-Source Sequence-to-Sequence Learning.**
*Jindřich Libovický, Jindřich Helcl.* ACL 2017.  [paper](http://aclweb.org/anthology/P17-2031)

* **Input Combination Strategies for Multi-Source Transformer Decoder.**
*Jindřich Libovický, Jindřich Helcl and David Marecek.* WMT 2018.  [paper](http://www.statmt.org/wmt18/pdf/WMT026.pdf)


### Model Improvement :

* **Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.**
*Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean.* 2016.  [paper](https://arxiv.org/pdf/1609.08144.pdf)

* **Non-Autoregressive Neural Machine Translation.**
*Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, Richard Socher.* ICLR 2018.  [paper](https://openreview.net/pdf?id=B1l8BtlCb)

* **Semi-Autoregressive Neural Machine Translation.**
*Chunqi Wang, Ji Zhang, Haiqing Chen.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1044)

* **Self-Attention with Relative Position Representations.**
*Peter Shaw, Jakob Uszkoreit, Ashish Vaswani.* ACL 2018.  [paper](http://aclweb.org/anthology/N18-2074)

* **Universal Transformers.**
*Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser.*  [paper](https://arxiv.org/pdf/1807.03819.pdf)

* **Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation.**
*He Tianyu, Tan Xu, Xia Yingce, He Di, Qin Tao, Chen Zhibo, Liu Tie-Yan.* NIPS 2018.  [paper](http://papers.nips.cc/paper/8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf)

* **The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.**
*Sameen Maruf, Gholamreza Haffari.* ACL 2018.  [paper](http://aclweb.org/anthology/P18-1008)

* **You May Not Need Attention.**
*Ofir Press, Noah A. Smith.*  2018.  [paper](https://arxiv.org/pdf/1810.13409.pdf)


#### Weight Tying:

* **Using the Output Embedding to Improve Language Models.**
*Ofir Press, Lior Wolf.* EACL 2017.  [paper](http://aclweb.org/anthology/E17-2025)

* **Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation.**
*Nikolaos Pappas, Lesly Miculicich Werlen, James Henderson.* WMT 2018.  [paper](http://aclweb.org/anthology/W18-6308)

#### Convolutional NMT:

* **Convolutional Sequence to Sequence Learning.**
*Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin.*  2017.  [paper](https://arxiv.org/pdf/1705.03122.pdf)

### Objective Function:

### Low Source:

* **Rapid Adaptation of Neural Machine Translation to New Languages.**
*Graham Neubig, Junjie Hu.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1103)

* **Copied Monolingual Data Improves Low-Resource Neural Machine Translation.**
*Anna Currey, Antonio Valerio Miceli Barone, and Kenneth Heafield.* WMT 2017.  [paper](http://www.aclweb.org/anthology/W17-4715)

### Multilingual NMT:

* **Contextual Parameter Generation for Universal Neural Machine Translation.**
*Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, Tom Mitchell.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1039)

* **Three Strategies to Improve One-to-Many Multilingual Translation.**
*Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu and Chengqing Zong.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1326)

### Unsupervised NMT:

### Incorporate Monolingual Data:

* **Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning.**
*Tobias Domhan and Felix Hieber.* EMNLP 2017.  [paper](http://aclweb.org/anthology/D17-1158)


#### Back-Translation:

* **Improving Neural Machine Translation Models with Monolingual Data.**
*Rico Sennrich, Barry Haddow, Alexandra Birch.* ACL 2016.  [paper](http://www.aclweb.org/anthology/P16-1009)

* **Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.**
*Marzieh Fadaee, Christof Monz.* EMNLP 2018.  [paper](http://aclweb.org/anthology/D18-1040)

* **Understanding Back-Translation at Scale.**
*Sergey Edunov, Myle Ott, Michael Auli, David Grangier.* EMNLP 2018.  [paper](https://aclanthology.info/papers/D18-1045/d18-1045)

### OOV Problem:

* **Neural Machine Translation of Rare Words with Subword Units.**
*Rico Sennrich and Barry Haddow and Alexandra Birch.* ACL 2016.  [paper](http://www.aclweb.org/anthology/P16-1162)

### Evaluation:

* **Statistical Significance Tests for Machine Translation Evaluation.**
*Philipp Koehn.* EMNLP 2004.  [paper](http://aclweb.org/anthology/W04-3250)

* **Clause Restructuring for Statistical Machine Translation.**
*Michael Collins, Philipp Koehn, Ivona Kucerova.* ACL 2005.  [paper](http://aclweb.org/anthology/P05-1066)

* **BLEU: a Method for Automatic Evaluation of Machine Translation.**
*Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu.* ACL 2002.  [paper](https://www.aclweb.org/anthology/P02-1040.pdf)

* **A Call for Clarity in Reporting BLEU Scores.**
*Matt Post.* WMT 2018.  [paper](http://aclweb.org/anthology/W18-6319)

